{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bl515-ml/Documents/shaio_jie/sma/Kpop_challenge_analyze/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import os\n",
    "from model import GNNRecommender\n",
    "from util import get_node_encoder, prepare_temporal_graph_data\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_node, hidden_channels=128, embedding_dim=64):\n",
    "        super(GATEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            GATConv(num_node, hidden_channels, heads=4),\n",
    "            torch.nn.LayerNorm(hidden_channels * 4),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            \n",
    "            GATConv(hidden_channels * 4, hidden_channels, heads=2),\n",
    "            torch.nn.LayerNorm(hidden_channels * 2),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            \n",
    "            GATConv(hidden_channels * 2, embedding_dim),\n",
    "            torch.nn.LayerNorm(embedding_dim),\n",
    "            torch.nn.ELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, GATConv):\n",
    "                x = layer(x, edge_index, edge_attr)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LinkClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_dim=64):\n",
    "        super(LinkClassifier, self).__init__()\n",
    "        \n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels * 2, 2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        h = torch.cat([u, v], dim=1)\n",
    "        return self.mlp(h).squeeze(1)\n",
    "        \n",
    "class LinkRegressor(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_dim=64):\n",
    "        super(LinkRegressor, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels * 2, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, u , v):\n",
    "        h = torch.cat([u, v], dim=1)\n",
    "        return self.mlp(h).squeeze(1)\n",
    "\n",
    "class GATLinkModel(torch.nn.Module):\n",
    "  def __init__(self, in_feats, hidden_feats = 128, embedding_dim = 64):\n",
    "    super().__init__()\n",
    "    self.encoder = GATEncoder(in_feats, hidden_feats, embedding_dim)\n",
    "    self.cls_head = LinkClassifier(hidden_feats, embedding_dim)\n",
    "    self.reg_head = LinkRegressor(hidden_feats, embedding_dim)\n",
    "    \n",
    "  def forward(self, x, edge_index, edge_attr, src_index, dst_index):\n",
    "    z = self.encoder(x, edge_index, edge_attr)  \n",
    "    src_z = z[src_index]\n",
    "    dst_z = z[dst_index]\n",
    "    \n",
    "    logits = self.cls_head(dst_z, src_z)\n",
    "    scores = self.reg_head(src_z, dst_z)\n",
    "    \n",
    "    return logits, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34717"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 k_pop_group_info 数据\n",
    "with open(\"/home/bl515-ml/Documents/shaio_jie/sma/Kpop_challenge_analyze/artist_texts.json\", \"r\") as f:\n",
    "    node_info = json.load(f)\n",
    "    \n",
    "max_length =0\n",
    "for node in node_info.keys():\n",
    "    if len(node_info[node]) > max_length:\n",
    "        max_length = len(node_info[node])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased').to('cuda')\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 获取节点编码器\n",
    "node_encoder = get_node_encoder()\n",
    "\n",
    "\n",
    "# 为每个节点生成 BERT embedding\n",
    "node_embeddings = {}\n",
    "for node in node_info.keys():\n",
    "    # 获取节点的描述文本\n",
    "    if len(node_info[node]) > 0:\n",
    "        text = node_info[node]\n",
    "    else:\n",
    "        text = node  # 如果没有描述，使用节点名称\n",
    "        \n",
    "    slice_embeddings = []\n",
    "    for slice_idx in range(0, len(text), 512):\n",
    "        slice_text = text[slice_idx:slice_idx+512]\n",
    "        # 使用 BERT 生成 embedding\n",
    "        inputs = tokenizer(f\"[CLS]{slice_text}[SEP]\", return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = inputs.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        slice_embeddings.append(outputs.last_hidden_state[:, 0, :].squeeze())\n",
    "        \n",
    "    # 使用 [CLS] token 的 embedding 作为节点特征\n",
    "    embedding = torch.mean(torch.stack(slice_embeddings), dim=0)\n",
    "    \n",
    "    node_embeddings[node.upper()] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9665e-01, -2.4611e-01,  1.2773e-03, -1.3177e-01, -3.3159e-03,\n",
       "         6.2177e-02, -1.0946e-01,  5.4648e-02, -1.9412e-01,  1.5306e-01,\n",
       "        -4.5052e-02,  1.5314e-01,  1.1436e-01,  2.8708e-01, -3.6588e-01,\n",
       "        -7.7544e-02,  1.2886e-01,  6.8055e-02,  1.5622e-01,  3.3720e-01,\n",
       "         3.1993e-02,  1.5004e-01, -4.6461e-02,  2.3265e-02,  1.8225e-01,\n",
       "        -1.9584e-01, -1.1333e-01,  2.5803e-01,  3.2474e-01, -4.6997e-02,\n",
       "         1.0414e-01,  1.8729e-01, -1.9658e-01,  2.2084e-01,  4.7247e-02,\n",
       "        -6.0015e-03, -1.1794e+00,  2.2172e-01,  7.6610e-02, -5.9659e-02,\n",
       "        -1.2987e-01, -1.9220e-02, -8.2201e-02,  4.4585e-03, -1.9403e-03,\n",
       "         7.8023e-01,  2.3173e-01,  2.1108e-01,  8.9115e-01,  1.9869e-01,\n",
       "         9.4542e-02, -3.4629e-01,  2.7083e-02, -1.0205e+00, -1.4013e-01,\n",
       "         7.2204e-02, -8.1762e-03, -2.0153e-01, -1.7985e-01,  8.4907e-02,\n",
       "         9.5718e-03, -1.3761e-01,  5.5709e-02, -4.9836e-02, -1.9562e-01,\n",
       "         7.0386e-02,  5.6497e-02,  3.2515e-02, -1.3240e-01,  1.2596e-01,\n",
       "         4.8970e-02,  6.8221e-02,  7.3786e-02, -9.0883e-03, -2.6473e-01,\n",
       "        -2.3361e-02,  1.5426e-01, -1.7672e-02, -1.1183e-01,  2.0248e-01,\n",
       "        -3.2696e-03, -1.4011e-02, -3.3875e-02,  9.6005e-02, -1.5595e-01,\n",
       "         7.7871e-02, -1.9415e-01, -1.9471e-02, -9.5301e-02, -1.3421e-01,\n",
       "        -2.7203e-02, -1.9046e-01, -1.9791e-01, -6.6297e-02, -1.6035e-01,\n",
       "        -2.3772e-01,  7.2903e-02,  2.1065e-02, -7.2695e-03,  9.2696e-02,\n",
       "        -2.1397e-01,  1.0691e+00,  2.9366e-02,  5.7229e-02, -1.8460e+00,\n",
       "        -4.3059e-02, -5.8179e-02, -1.1826e-01,  2.5729e-01, -1.0852e-01,\n",
       "         2.0881e-01, -1.6204e-02,  3.3305e-01, -7.2164e-02, -8.8866e-02,\n",
       "         2.2636e-02, -1.5461e-01, -9.7943e-02,  7.3410e-02, -9.4984e-02,\n",
       "        -2.5003e-01, -1.0788e-01,  1.0076e-01, -9.1103e-03, -2.2513e-01,\n",
       "        -3.5185e-01,  9.5652e-02,  6.1576e-02,  2.1704e-01,  5.2073e-01,\n",
       "         2.1298e-01,  1.1816e+00,  2.6450e-01, -1.3387e-01, -2.1050e-01,\n",
       "         9.8725e-01,  6.8186e-02,  2.5486e-01,  2.8531e-01,  5.6393e-01,\n",
       "        -5.1222e-03, -4.7704e-02, -4.9042e-01,  2.2450e-02, -1.8604e-01,\n",
       "        -1.6089e-01, -5.1396e-02,  2.8127e-01,  8.2602e-02, -1.5352e-01,\n",
       "         7.6433e-02, -2.2420e-01,  3.6923e-02, -7.1933e-02, -4.8383e-02,\n",
       "         9.2948e-02,  9.9351e-02, -2.3225e-01,  1.4248e-01,  2.6719e-01,\n",
       "         1.0176e+00,  5.8804e-02, -3.3859e-01,  6.9038e-02, -2.9007e-01,\n",
       "        -1.4141e-01,  3.4293e-02, -2.5763e-01, -1.9569e-01, -1.7404e-01,\n",
       "        -1.8376e-01, -3.1564e-01, -5.6950e-02,  1.2491e-01,  1.3968e-01,\n",
       "        -1.2269e-01, -1.1762e-01, -5.0386e-02,  3.8576e-02, -2.1059e-01,\n",
       "        -1.8606e-01, -2.3074e-02, -3.9471e-02,  1.8294e-01,  1.0714e-01,\n",
       "        -2.1070e-01, -2.4434e-02,  6.9233e-03, -1.0799e-01, -2.3321e-01,\n",
       "         1.1432e-01,  8.8902e-02,  1.3048e-01, -3.9390e-02,  1.1535e-02,\n",
       "         8.2323e-02,  1.1674e-02,  1.4279e-01,  7.4054e-02, -1.6195e-02,\n",
       "         3.3275e-01,  8.6109e-02, -1.6861e-02, -1.2643e-01, -1.8254e-01,\n",
       "         4.9768e-01,  7.7668e-02, -2.4512e-02,  1.0584e-01,  1.7169e-01,\n",
       "         7.6821e-02,  1.1512e-01,  6.0538e-01,  9.2589e-03,  9.2913e-02,\n",
       "         6.2288e-02,  4.1862e-02,  2.3090e-02,  2.1204e-01, -1.1968e+00,\n",
       "        -3.4438e-02, -9.3754e-02,  1.0934e-01,  1.3769e-01, -2.3484e-01,\n",
       "         1.4097e-01, -1.5943e-01,  1.7854e-01,  3.3469e-02, -3.3851e-01,\n",
       "         2.9866e-01, -2.2114e-01, -8.6746e-02, -1.0034e+00, -2.6654e-01,\n",
       "         1.2652e-01,  1.9432e-01, -3.3291e-01, -2.2424e-01, -6.1040e-02,\n",
       "         3.4927e-02,  1.9805e-01, -6.9818e-02, -1.1514e-01,  2.8164e-01,\n",
       "         6.9427e-02,  7.3134e-01,  5.7616e-02,  6.5225e-02,  9.1376e-03,\n",
       "         1.2527e-01, -3.1613e-02,  4.6282e-01,  7.7640e-02,  1.1758e-01,\n",
       "         1.8450e-01,  1.4359e-01, -4.1696e-03,  2.5722e-01, -1.0910e-01,\n",
       "        -1.0665e-01, -1.9456e-01,  2.8472e-04, -1.9690e-01, -8.9724e-02,\n",
       "        -9.4932e-02,  1.0832e-01, -7.7703e-03,  1.2520e-01, -2.0468e-01,\n",
       "        -1.1075e-01, -2.8785e-02, -5.8459e-02,  1.7899e-01,  1.8156e-03,\n",
       "         1.2163e-01,  4.9133e-02,  1.0262e-01, -5.4323e-02,  3.2027e-02,\n",
       "        -5.7292e-02, -9.5506e-02, -2.2566e-02, -2.3706e-02,  3.7285e-02,\n",
       "         1.5193e-02, -5.0447e-02, -1.9206e-01, -1.6154e-01, -6.4382e-01,\n",
       "        -3.6922e-01,  8.8674e-03, -2.8908e-02, -2.6890e-01, -8.3607e-02,\n",
       "        -1.9342e-01, -2.0220e-01,  4.0477e-02,  7.1652e-02,  9.7922e-02,\n",
       "         3.6725e-01, -1.3189e-01,  1.0437e-01,  2.2760e-02, -1.0073e+00,\n",
       "        -3.5437e-03,  1.9760e-02,  5.4983e-02,  1.7787e-01,  1.0925e-01,\n",
       "        -5.0813e-02,  1.1131e-01,  7.3983e-02, -1.2180e-01, -3.5400e-01,\n",
       "         4.9014e-04,  6.8892e-02,  1.5012e-01, -3.2058e-02,  1.8227e-01,\n",
       "        -2.4051e-01, -1.0645e+00,  1.9820e-01, -9.9718e-02, -8.6787e-02,\n",
       "        -3.5582e-02, -3.1219e-02,  3.4638e-02,  9.5631e-02,  7.2234e-02,\n",
       "         1.1008e-01,  5.7555e-01,  1.0245e-01, -2.1530e-01,  5.9081e-02,\n",
       "         1.2847e-01, -2.3327e-02, -1.2688e-01, -1.2251e-01, -1.7438e-01,\n",
       "        -8.8424e-02, -1.8592e-01,  1.6992e-01,  4.2402e-02, -8.3396e-02,\n",
       "        -3.6829e-02,  2.1625e-02,  1.5610e-01,  1.0207e-01,  7.2577e-02,\n",
       "         1.7043e-01,  1.4322e-01, -3.0463e-01,  2.0633e-02,  5.5483e-02,\n",
       "        -2.4234e-01, -1.3508e-01,  1.7734e-01,  1.6542e-01,  2.2353e-01,\n",
       "        -1.5557e-01,  2.2009e-01,  2.9233e-02,  2.7630e-02,  5.3649e-02,\n",
       "         2.0547e-01,  4.3570e-02, -4.8359e-03, -6.7525e-02,  1.8818e-01,\n",
       "         2.4765e-01,  4.6425e-01, -3.5775e-01, -4.7002e-01, -5.3918e-02,\n",
       "         1.2909e-02,  1.2360e-01, -1.7923e-01,  2.9905e-01,  9.4407e-02,\n",
       "         2.1440e-01,  7.3912e-02, -1.2085e+00, -3.5706e-02, -5.8981e-02,\n",
       "        -6.2422e-01, -1.3556e-01, -1.2495e-01,  6.3080e-03, -9.7133e-02,\n",
       "         5.6895e-02, -8.2202e-02, -1.2538e-01, -1.2978e-02,  2.5234e-02,\n",
       "         4.1522e-02,  1.4372e-01,  4.2605e-02,  7.3872e-02,  8.5315e-02,\n",
       "         2.0289e-01, -2.2551e-01,  3.5366e-01,  1.8592e-01, -5.1447e-02,\n",
       "        -1.0759e+00,  3.0180e-01,  1.3169e-01, -2.7186e-02, -5.3639e-02,\n",
       "         3.5956e-02,  1.8379e-01,  2.0818e-01,  1.6727e-01,  5.3562e-02,\n",
       "         3.6820e-01, -3.1189e-01,  1.1520e-01,  2.7002e-01, -7.3787e-02,\n",
       "         2.8475e-01,  4.8033e-01, -1.2524e-01, -3.9563e-01, -2.2540e-01,\n",
       "        -7.1167e-02,  7.5982e-02, -2.2019e-01,  1.5823e-02, -9.7998e-02,\n",
       "        -3.5096e-01, -1.2400e-01,  5.0215e-02, -9.1227e-01, -1.1884e+00,\n",
       "         4.6116e-01,  4.0662e-02,  4.0883e-02,  2.0983e-02,  1.2004e+00,\n",
       "         3.2003e-02,  1.7837e-01,  1.5482e-02,  1.2434e-01, -1.2155e-01,\n",
       "         2.5333e+00,  2.8179e-01, -2.2302e-01,  8.6798e-01, -1.0719e-02,\n",
       "         1.3686e-01, -3.5045e-01, -8.7423e-01, -1.2666e+00, -5.6394e-02,\n",
       "        -2.2652e-01, -2.3811e-01, -1.4706e-01,  1.0530e-02, -1.3387e-01,\n",
       "        -5.4580e-02,  1.1428e-01,  1.7164e-01,  6.6554e-02, -1.5583e-01,\n",
       "        -1.9822e-01, -1.9051e-01, -3.4893e-02,  2.4300e-02,  2.6703e-01,\n",
       "         3.6260e-01,  1.4276e-01, -1.6450e-01, -2.8401e-02, -2.6982e-01,\n",
       "        -1.2418e+00,  7.6862e-02,  1.0618e-01,  1.7179e-01, -2.9663e-01,\n",
       "        -9.1611e-03,  8.4722e-02,  4.1090e-01, -2.2145e-01,  2.1200e-01,\n",
       "        -1.0490e-02, -8.3351e-02, -5.2605e-02, -8.6278e-02,  1.5347e-01,\n",
       "         6.8028e-02, -8.9159e-02, -2.6924e-02,  1.1370e+00, -2.5761e-02,\n",
       "         5.1590e-03, -1.0535e-01,  1.3261e-02,  1.5003e-01, -2.8066e-01,\n",
       "        -9.3393e-02,  5.7137e-02,  1.8280e-01,  2.6107e-03, -1.1512e-01,\n",
       "        -1.8653e-02, -4.2746e-01,  2.2377e-01, -9.0810e-02, -1.3093e-01,\n",
       "         3.3161e-01,  3.3715e-02,  1.5244e-01, -3.7513e-01,  4.1412e-02,\n",
       "        -1.6229e-02,  3.3179e-02, -1.4333e-02, -9.8476e-02,  6.5383e-02,\n",
       "        -9.3002e-02,  1.6342e-01,  2.3305e-01, -8.9391e-02, -1.7859e-01,\n",
       "        -2.7821e-01, -2.9609e-01, -1.4129e-01, -9.8423e-02,  1.0707e-01,\n",
       "         2.8412e-02, -3.5853e-01, -1.9981e-01,  9.9227e-01, -6.6202e-02,\n",
       "         1.2773e-01,  6.5696e-01, -1.2263e-01, -2.2942e-01, -4.2087e-02,\n",
       "         1.2070e-01,  3.8511e-02, -2.8562e-01, -1.5030e-01,  5.1788e-02,\n",
       "         2.9233e-01,  1.2948e-01,  2.7774e-02,  5.7158e-02, -1.4805e-03,\n",
       "         1.9257e-01,  2.1449e-01,  1.5658e-01,  5.1688e-02, -1.5889e-01,\n",
       "        -1.4110e-01,  8.4296e-03,  3.8448e-02, -1.3294e-01, -4.7705e-02,\n",
       "         1.7643e-01,  4.9192e-02, -4.4661e-03, -2.0899e-01, -3.4286e-02,\n",
       "        -7.6092e-02, -6.8388e-02,  1.2472e-01, -7.2926e-02,  6.1589e-02,\n",
       "         6.9119e-02,  1.8419e-01,  1.9511e-01, -5.8415e-03,  1.6601e-02,\n",
       "        -7.3015e-02, -3.4060e-02,  2.5147e-01,  1.6703e-01,  1.0629e-01,\n",
       "         1.6687e-01, -3.7923e-01, -7.3177e-02, -1.1230e-02, -2.7533e-01,\n",
       "        -1.0812e-01, -9.2951e-02,  2.8957e-01, -3.0235e-01, -1.2202e-01,\n",
       "         1.2405e-01,  1.1092e-01, -1.6608e-01, -2.5274e-02,  1.7432e-01,\n",
       "         2.8200e-01, -6.9810e-02,  1.0687e-01,  1.1386e-01, -9.6544e-02,\n",
       "         2.3436e-01, -9.5769e-02,  1.9675e-01,  7.0425e-02, -1.9655e-01,\n",
       "        -7.0128e-02, -9.8375e-02,  1.5423e-01, -4.5237e-01,  1.2331e-01,\n",
       "         2.1031e-01, -5.2575e-02, -7.0587e-03,  3.6144e-02, -2.3314e-01,\n",
       "        -1.1722e+00,  1.0841e-02,  1.0135e-01,  1.4658e-01, -1.3487e-02,\n",
       "        -4.6872e-02,  8.2774e-01, -1.3283e-01,  6.6568e-02, -1.3217e-02,\n",
       "        -1.2233e-01, -1.5749e-01, -6.1971e-02,  4.8057e-02, -3.6472e-01,\n",
       "         1.4112e-01, -1.8143e-01,  8.8351e-02, -2.0442e-02,  5.3159e-02,\n",
       "        -8.9202e-01, -3.0033e-01, -2.9719e-01,  6.0874e-02,  5.5889e-03,\n",
       "        -1.1479e-01,  2.3381e-01, -8.2854e-02, -2.1225e-01,  1.0405e-01,\n",
       "         7.6063e-02, -1.3602e-01,  4.1806e-02, -1.9731e-01, -4.5408e-02,\n",
       "         4.5354e-02, -1.5725e-01,  6.3447e-02, -1.0680e-02, -6.9682e-02,\n",
       "        -1.1886e-02, -1.8002e-01, -2.8719e-02, -1.1593e-01,  3.0055e-01,\n",
       "         1.9637e-01,  1.4866e-01,  3.0594e-02, -7.6817e-02,  5.3791e-02,\n",
       "        -8.3227e-01,  8.8669e-02, -6.1817e-02,  1.2880e-01, -1.7830e-01,\n",
       "         2.8177e-01, -1.2687e-02, -1.3466e-01, -7.5486e-02,  1.2970e-01,\n",
       "        -3.0069e-01,  1.2360e-01,  2.8681e-02,  2.0238e-01,  5.0547e-02,\n",
       "         1.0126e-02,  3.3130e-01,  2.7009e-01, -8.0254e-02,  4.5881e-03,\n",
       "        -6.7662e-02, -8.2009e-02,  3.1332e-01, -2.3525e-01,  3.3240e-01,\n",
       "         2.6383e-01,  7.3639e-02,  1.9900e-01, -8.6126e-02, -1.3161e-01,\n",
       "         1.3096e-01, -6.6517e-01,  6.7838e-02,  8.1465e-02, -9.0257e-02,\n",
       "         8.7883e-02, -3.4015e-01,  2.9201e-01,  2.8277e-02,  1.1649e-01,\n",
       "         1.5984e-03,  2.7916e-01,  9.6708e-01,  1.8091e-01,  1.4724e-01,\n",
       "         2.6513e-01, -1.1556e-01, -2.5547e-01,  2.3897e-01, -2.5315e-01,\n",
       "         9.6231e-01,  1.4780e-02, -3.5339e-01,  8.0961e-02,  3.1395e-01,\n",
       "         1.7683e-01,  1.3889e+00, -2.5291e-01, -2.0168e-01,  4.9898e-02,\n",
       "         1.1113e-01,  1.4834e-01, -1.1112e-01, -6.0842e-03, -1.9468e-02,\n",
       "        -1.1023e-02, -1.3490e+00,  2.0384e-01,  1.0684e-01,  3.9733e-01,\n",
       "        -1.5867e-01, -3.0721e-01,  1.4627e-01,  1.1429e-01,  1.5457e-01,\n",
       "        -1.3505e-01, -1.4916e-01, -7.2615e-02, -5.6530e-03,  1.7068e-01,\n",
       "         1.1502e-01, -1.7473e-01, -1.0356e-01,  1.1119e-01, -2.2594e-01,\n",
       "        -1.7902e-01, -2.1627e-01,  3.3620e-01, -4.3567e-02,  1.3567e-01,\n",
       "         3.5822e-01, -1.1740e-01, -3.8224e-02,  9.8539e-02,  4.7350e-03,\n",
       "        -2.2616e-01, -2.1132e-01,  1.3284e-01, -2.8149e-02, -1.2208e-02,\n",
       "         4.8542e-01,  1.2687e-02, -1.3432e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = torch.zeros(node_encoder.classes_.shape[0], 768)\n",
    "for i in node_encoder.classes_:\n",
    "    name = i.split('_')[-1]\n",
    "    if name == 'BAHIYYIH':\n",
    "        name = 'HUENING_BAHIYYIH'\n",
    "    elif name == 'KI':\n",
    "        name = 'NI_KI'\n",
    "    elif name == 'KIM':\n",
    "        name = \"E_TION\"\n",
    "    elif name == 'N':\n",
    "        name = \"I_N\"\n",
    "        \n",
    "    if name in node_embeddings:\n",
    "        node_features[node_encoder.transform([i])[0]] = node_embeddings[name]\n",
    "    else:\n",
    "        node_features[node_encoder.transform([i])[0]] = torch.zeros(768)\n",
    "        \n",
    "node_features[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "model = GATLinkModel(768).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# 設定 early stopping 參數\n",
    "patience = 10\n",
    "min_delta = 0.001\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "WINDOW_SIZE = 180\n",
    "REMOVED_SAME_GROUP = False\n",
    "\n",
    "# 加載數據\n",
    "collaboration_df = pd.read_csv(\"/home/bl515-ml/Documents/shaio_jie/sma/Kpop_challenge_analyze/data/collaboration_videos.csv\")\n",
    "collaboration_df['timestamp'] = collaboration_df['timestamp'].astype(float)\n",
    "\n",
    "# 只使用不同組的藝人\n",
    "if REMOVED_SAME_GROUP:\n",
    "    collaboration_df = collaboration_df[collaboration_df['source'].str.split('_').str[0] != collaboration_df['target'].str.split('_').str[0]]\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    end_time = collaboration_df['timestamp'].min()\n",
    "    offset = 10\n",
    "    total_iter = 0\n",
    "    while end_time < collaboration_df['timestamp'].max():\n",
    "        start_time = end_time - WINDOW_SIZE * 24 * 3600\n",
    "        window_data = collaboration_df[\n",
    "            (collaboration_df['timestamp'] >= start_time) & \n",
    "            (collaboration_df['timestamp'] < end_time)\n",
    "        ]\n",
    "        \n",
    "        offset += window_data.shape[0]\n",
    "        \n",
    "        if offset >= collaboration_df.shape[0]:\n",
    "            break\n",
    "        \n",
    "        end_time = collaboration_df['timestamp'].iloc[offset]\n",
    "        \n",
    "        if len(window_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        # 創建邊索引和特徵\n",
    "        edge_index = []\n",
    "        edge_feature = []\n",
    "        scores = []\n",
    "        \n",
    "        \n",
    "        for _, row in window_data.iterrows():\n",
    "            artist1_idx = node_encoder.transform([row['source']])[0]\n",
    "            artist2_idx = node_encoder.transform([row['target']])[0]\n",
    "                \n",
    "            edge_index.append([artist1_idx, artist2_idx])\n",
    "                \n",
    "            # 計算邊特徵\n",
    "            views = row['views'] if 'views' in row else 0\n",
    "            likes = row['likes'] if 'likes' in row else 0\n",
    "            comments = row['comments'] if 'comments' in row else 0\n",
    "\n",
    "            # 組合特徵\n",
    "            feature = [ views, likes, comments]\n",
    "            edge_feature.append(feature)\n",
    "            scores.append(row['views'] * 0.5 + row['likes'] * 0.3 + row['comments'] * 0.2)\n",
    "            \n",
    "        \n",
    "        # 轉換為張量\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_feature = torch.tensor(edge_feature, dtype=torch.float)\n",
    "        scores = torch.tensor(scores, dtype=torch.float).t()\n",
    "        # 創建圖數據\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_feature, scores=scores).to('cuda')\n",
    "        \n",
    "        # 使用 LinkNeighborLoader 進行正負取樣\n",
    "        loader = LinkNeighborLoader(\n",
    "            data=data,\n",
    "            num_neighbors=[10, 10],  # 每個節點採樣10個一階鄰居和10個二階鄰居\n",
    "            batch_size=32,\n",
    "            edge_label_index=edge_index,  # 正樣本邊\n",
    "            neg_sampling_ratio=1.0,  # 負樣本比例為1:1\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # 訓練模型\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for batch in loader:\n",
    "            batch = batch.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向傳播\n",
    "            cl, reg = model(data.x, data.edge_index, data.edge_attr, batch.edge_label_index[0].t(), batch.edge_label_index[1].t())\n",
    "            \n",
    "            # 計算損失\n",
    "            # 計算分類損失\n",
    "            classification_loss = F.cross_entropy(\n",
    "                cl, \n",
    "                batch.edge_label.long()\n",
    "            )\n",
    "            \n",
    "            # print(batch)\n",
    "            # 根據 input_id 獲取對應的 scores\n",
    "            mask_pos = batch.edge_label == 1\n",
    "            # 計算正樣本的 scores\n",
    "            mask_reg = reg[mask_pos]\n",
    "            # 計算回歸損失 (預測互動分數)\n",
    "            regression_loss = F.mse_loss(\n",
    "                mask_reg, \n",
    "                data.scores[batch.input_id]\n",
    "            )\n",
    "            \n",
    "            # 組合兩種損失\n",
    "            loss =  classification_loss +  0.01 * regression_loss\n",
    "\n",
    "            # 反向傳播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_iter += 1\n",
    "        \n",
    "    avg_loss = epoch_loss / total_iter\n",
    "    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Early stopping 檢查\n",
    "    if avg_loss < best_loss - min_delta:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), 'best_model_v2_180.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "        break\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加載最佳模型\n",
    "model.load_state_dict(torch.load('best_model_v2_180.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "519it [09:41,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# 只使用歷史數據\n",
    "test_itter = collaboration_df[collaboration_df['timestamp'] >= datetime(2025, 1, 1).timestamp()]\n",
    "\n",
    "all_recommendations = []\n",
    "# 創建時序數據加載器\n",
    "test_snapshots = []\n",
    "for _, itter_row in tqdm(test_itter.iterrows()):\n",
    "    start_time = itter_row['timestamp'] - WINDOW_SIZE * 24 * 3600\n",
    "    # 獲取當前窗口內的數據\n",
    "    window_data = collaboration_df[\n",
    "        (collaboration_df['timestamp'] >= start_time) & \n",
    "        (collaboration_df['timestamp'] < itter_row['timestamp'])\n",
    "    ]\n",
    "        \n",
    "    if len(window_data) > 0:\n",
    "        # 創建邊索引和特徵\n",
    "        edge_index = []\n",
    "        edge_feature = []\n",
    "        scores = []\n",
    "            \n",
    "        for _, row in window_data.iterrows():\n",
    "            artist1_idx = node_encoder.transform([row['source']])[0]\n",
    "            artist2_idx = node_encoder.transform([row['target']])[0]\n",
    "                \n",
    "            edge_index.append([artist1_idx, artist2_idx])\n",
    "                \n",
    "            # 計算邊特徵\n",
    "            views = row['views'] if 'views' in row else 0\n",
    "            likes = row['likes'] if 'likes' in row else 0\n",
    "            comments = row['comments'] if 'comments' in row else 0\n",
    "\n",
    "            # 組合特徵\n",
    "            feature = [ views, likes, comments]\n",
    "            edge_feature.append(feature)\n",
    "            scores.append(row['views'] * 0.5 + row['likes'] * 0.3 + row['comments'] * 0.2)\n",
    "            \n",
    "        # 轉換為張量\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_feature = torch.tensor(edge_feature, dtype=torch.float)\n",
    "\n",
    "        \n",
    "        # 創建圖數據\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_feature, scores=scores).to('cuda')\n",
    "        \n",
    "        \n",
    "    \n",
    "        source = itter_row['source']\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            source_idx = node_encoder.transform([source])\n",
    "            for j in node_encoder.classes_:\n",
    "                if j == source:\n",
    "                    continue\n",
    "                target_idx = node_encoder.transform([j])\n",
    "                cl, reg = model(data.x, data.edge_index, data.edge_attr, source_idx, target_idx)\n",
    "            \n",
    "                softmax_cl = F.softmax(cl, dim=1)\n",
    "            \n",
    "            \n",
    "                is_positive = softmax_cl[:,1].item() > 0.5\n",
    "            \n",
    "                if is_positive:\n",
    "                    predictions.append((j, softmax_cl[:,1].item(), reg.item(), is_positive))\n",
    "\n",
    "    \n",
    "    \n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        top_k_recommendations = predictions[:10]\n",
    "    \n",
    "        top_k_recommendations = top_k_recommendations.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "        all_recommendations.append({\n",
    "            'source': source,\n",
    "        'label': itter_row['target'],\n",
    "        'score': itter_row['views'] * 0.5 + itter_row['likes'] * 0.3 + itter_row['comments'] * 0.2,\n",
    "        'recommendations': predictions\n",
    "    })\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOYNEXTDOOR_SUNGHO'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_itter.iloc[0]['source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 7.9876 %, Hit@10: 21.9653 %, MSE: 659.1313\n"
     ]
    }
   ],
   "source": [
    "# 計算評估指標\n",
    "true_positives = 0\n",
    "total_recommendations = 0\n",
    "reciprocal_ranks = []\n",
    "hits_at_k = 0\n",
    "mses = []\n",
    "    \n",
    "for recommendations in all_recommendations:\n",
    "    rank = 0\n",
    "    selected_reg = 0\n",
    "    for i,(n, cl, reg, is_positive) in enumerate(recommendations['recommendations']):\n",
    "        if n == recommendations['label']:\n",
    "            rank = i + 1\n",
    "            selected_reg = reg\n",
    "            break;\n",
    "    \n",
    "    if rank > 0:\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        mses.append((selected_reg - recommendations['score']) ** 2)\n",
    "        if rank <= 10:\n",
    "            hits_at_k += 1\n",
    "    else:\n",
    "        reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    total_recommendations += len(recommendations['recommendations'])\n",
    "    \n",
    "        \n",
    "mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "hit_at_k = hits_at_k / len(all_recommendations) if len(all_recommendations) > 0 else 0\n",
    "\n",
    "\n",
    "mrr_percentage = mrr * 100\n",
    "hit_at_k_percentage = hit_at_k * 100\n",
    "print(f\"MRR: {mrr_percentage:.4f} %, Hit@10: {hit_at_k_percentage:.4f} %, MSE: {np.mean(mses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "預測結果已保存至: predictions/predictions_eee.csv\n"
     ]
    }
   ],
   "source": [
    "last_df = collaboration_df[collaboration_df['timestamp'] >= collaboration_df['timestamp'].max() - WINDOW_SIZE * 24 * 3600]\n",
    "\n",
    "result = []\n",
    "edge_index=[]\n",
    "edge_feature=[]\n",
    "scores=[]\n",
    "for _, row in last_df.iterrows():\n",
    "        \n",
    "    artist1_idx = node_encoder.transform([row['source']])[0]\n",
    "    artist2_idx = node_encoder.transform([row['target']])[0]\n",
    "                \n",
    "    edge_index.append([artist1_idx, artist2_idx])\n",
    "                \n",
    "    # 計算邊特徵\n",
    "    views = row['views'] if 'views' in row else 0\n",
    "    likes = row['likes'] if 'likes' in row else 0\n",
    "    comments = row['comments'] if 'comments' in row else 0\n",
    "\n",
    "    # 組合特徵\n",
    "    feature = [ views, likes, comments]\n",
    "    edge_feature.append(feature)\n",
    "    scores.append(row['views'] * 0.5 + row['likes'] * 0.3 + row['comments'] * 0.2)\n",
    "            \n",
    "# 轉換為張量\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_feature = torch.tensor(edge_feature, dtype=torch.float)\n",
    "    \n",
    "        \n",
    "# 創建圖數據\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_feature, scores=scores).to('cuda')\n",
    "    \n",
    "source = itter_row['source']\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in node_encoder.classes_:\n",
    "        for j in node_encoder.classes_:\n",
    "            if j == i:\n",
    "                continue\n",
    "            target_idx = node_encoder.transform([j])\n",
    "            cl, reg = model(data.x, data.edge_index, data.edge_attr, source_idx, target_idx)\n",
    "            \n",
    "            softmax_cl = F.softmax(cl, dim=1)\n",
    "            \n",
    "            is_positive = softmax_cl[:,1].item() > 0.5\n",
    "            \n",
    "            if is_positive:\n",
    "                predictions.append((j, softmax_cl[:,1].item(), reg.item(), is_positive))\n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        top_k_recommendations = predictions[:10]\n",
    "    \n",
    "        top_k_recommendations = top_k_recommendations.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "        result.append({\n",
    "            'source': i,\n",
    "            'recommendations': predictions\n",
    "        })\n",
    "    \n",
    "# 創建預測結果 DataFrame\n",
    "    predictions_df = pd.DataFrame(result)\n",
    "    \n",
    "    # 保存為 CSV 文件\n",
    "    output_dir = 'predictions'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_file = os.path.join(output_dir, 'predictions_eee.csv')\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n預測結果已保存至: {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
